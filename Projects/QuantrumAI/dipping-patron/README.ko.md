# Patron - 차트 패턴 유사도 검색 시스템 (데이터 전처리)

## 🎯 프로젝트 배경

**문제점**: 주식 초보자(주린이)가 복잡한 기술적 지표 없이 차트 패턴을 학습하기 어려움

**해결책**: 사용자가 업로드한 주식 차트 이미지와 유사한 과거 패턴을 찾아, 해당 패턴 이후의 주가 흐름을 보여주는 AI 기반 교육 도구

**핵심 가치**: "과거의 비슷한 패턴을 보고 배우자"

---

## 🛠 기술 스택

### 데이터 수집 및 전처리
- **Python**: pandas, numpy
- **데이터 소스**: yfinance API
- **이미지 처리**: Pillow, OpenCV
- **정규화**: scikit-learn (MinMaxScaler)
- **시각화**: matplotlib, mplfinance

### AI/ML (다음 단계)
- PyTorch (ResNet18)
- Triplet Loss (Semi-hard Negative Mining)
- Faiss (유사도 검색)

### 백엔드/프론트엔드 (다음 단계)
- Django 5.1 + PostgreSQL
- React 19 + TypeScript

---

## 💻 내 역할

**전체 프로젝트 전담** (1부터 100까지)
- 프로젝트 기획 및 설계
- 데이터 수집 및 전처리 (현재 완료)
- AI 모델 개발 (예정)
- 백엔드/프론트엔드 개발 (예정)

---

## 🔧 핵심 구현 내용

### 1. 데이터 수집 (yfinance API)

**종목 선정**:
```python
NASDAQ 100 + S&P 100 → 중복 제거 → 172개 종목
```

**수집 설정**:
- 기간: 2020-01-01 ~ 2025-10-29 (약 6년)
- 간격: 주봉 (1wk)
- 보정: auto_adjust=True (액면분할 자동 보정)

**결과**: 172개 종목 × 305주 = 52,360개 데이터포인트

### 2. 슬라이딩 윈도우 패턴 생성

**윈도우 크기**: 12주

**슬라이딩 방식**:
```
305주 데이터 → 12주씩 슬라이딩
= (305 - 12 + 1) = 294개 패턴/종목
```

**최종 패턴 수**: 172종목 × 294패턴 = **49,815개 패턴**

### 3. 데이터 정규화

**정규화 전략**: 각 12주 패턴마다 독립적으로 MinMaxScaler 적용

```python
# 각 패턴을 0~1로 정규화
scaler = MinMaxScaler()
normalized_pattern = scaler.fit_transform(pattern)
```

**이유**: 절대 가격이 아닌 상대적 패턴 형태 학습

### 4. 메타데이터 생성

**메타데이터 컬럼**:
```csv
ticker, pattern_id, start_date, end_date, start_price, end_price,
sector, industry, return_3m, return_6m, return_1y
```

**이후 수익률 계산**:
- 3개월 후 수익률 (return_3m)
- 6개월 후 수익률 (return_6m)
- 12개월 후 수익률 (return_1y)

### 5. 그레이스케일 차트 이미지 생성

**변환 과정**:
```
(12주, 4열) 숫자 배열 → (224, 224, 1) 그레이스케일 이미지
```

**이미지 형식**:
- 크기: 224×224 (ResNet18 입력 크기)
- 채널: 그레이스케일 (1채널)
- 정규화: 0~1 범위

---

## 📊 데이터 통계

| 항목 | 값 |
|------|-----|
| **총 종목 수** | 172개 |
| **데이터 기간** | 2020-01-01 ~ 2025-10-29 (약 6년) |
| **총 주봉 수** | 305주 |
| **생성된 패턴 수** | 49,815개 |
| **데이터 용량** | ~6 MB (CSV + 메타데이터) |

**데이터 특징**:
- ✅ 2020년 코로나 변동성 포함 (다양한 패턴)
- ✅ 액면분할 보정 완료
- ⚠️ 주봉만 (일봉/분봉 없음)
- ⚠️ 미국 주식만 (한국 주식 없음)

---

## 💡 배운 점

### 1. 금융 데이터 다루기
- yfinance API 사용법
- 주가 데이터 특성 (OHLC, 액면분할)
- 시계열 데이터 슬라이딩 윈도우 기법

### 2. 데이터 전처리 전략
- 패턴별 독립 정규화의 중요성
- 메타데이터 설계 (이후 수익률 계산)
- 대용량 데이터 효율적 처리

### 3. AI 모델 설계
- ResNet18 입력 형식 이해 (224×224)
- 그레이스케일 vs RGB 트레이드오프
- Triplet Loss를 위한 데이터 구조 설계

---

## 🔗 다음 단계

### Phase 2: AI 모델 학습 (예정)
- [ ] ResNet18 백본 구축 (ImageNet 전이학습)
- [ ] Triplet Loss + Semi-hard Negative Mining 구현
- [ ] 학습 데이터셋 구성 (Anchor-Positive-Negative)
- [ ] 모델 학습 및 검증

### Phase 3: 유사도 검색 (예정)
- [ ] Faiss 인덱스 구축 (L2 Distance)
- [ ] 시간 다양성 필터링 알고리즘
- [ ] Django API 엔드포인트 개발

### Phase 4: 프론트엔드 (예정)
- [ ] React UI 개발 (5×3 행렬 출력)
- [ ] 차트 시각화 (Recharts)
- [ ] 통합 테스트

---

## ❓ 면접 예상 질문

**Q1: 왜 주봉 데이터를 선택했나요?**
> A: 일봉은 데이터량이 너무 많아 학습 비용이 크고, 월봉은 패턴 수가 적어 다양성이 부족합니다. 주봉은 6년 데이터로 49,815개의 충분한 패턴을 확보하면서도 계산 비용이 적당한 균형점입니다.

**Q2: 각 패턴마다 독립적으로 정규화한 이유는?**
> A: 절대 가격(예: AAPL $150 vs TSLA $200)이 아닌 상대적 패턴 형태(예: 상승 추세, 하락 추세)를 학습시키기 위해서입니다. 가격이 다른 종목 간에도 패턴 유사성을 비교할 수 있습니다.

**Q3: Semi-hard Negative Mining을 선택한 이유는?**
> A: Easy Negative는 학습 효과가 낮고, Hard Negative는 학습이 불안정합니다. Semi-hard Negative는 "적당히 헷갈리는" 샘플로 학습 효율을 최대화합니다.

**Q4: 49,815개 패턴이 충분한가요?**
> A: Triplet Loss는 한 배치에서 여러 조합을 만들기 때문에, 실질적인 학습 샘플은 훨씬 많습니다. 또한 ImageNet 전이학습으로 시작하므로 충분한 데이터량입니다.

**Q5: 다음 단계에서 가장 중요한 작업은?**
> A: Triplet Loss의 Semi-hard Negative Mining 구현입니다. 이것이 제대로 작동해야 유사한 패턴끼리 가깝게, 다른 패턴끼리 멀게 임베딩됩니다.

---

## 📂 프로젝트 구조
```
Patron/
├── data/
│   ├── raw/                 # yfinance 원본 데이터
│   ├── processed/           # 전처리된 패턴 (49,815개)
│   ├── metadata.csv         # 메타데이터
│   └── images/              # 그레이스케일 차트 이미지
├── notebooks/
│   └── preprocessing.ipynb  # 전처리 코드
└── README.md                # 프로젝트 설명
```

---

**현재 상태**: ✅ Phase 1 완료 (데이터 전처리)
**다음 목표**: 🔄 Phase 2 진행 중 (AI 모델 학습)
